{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wstachyr\\Documents\\studia\\PJA\\DS\\S3\\INL\\projekt\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'\n",
    "import pandas as pd\n",
    "# from simpletransformers.ner import NERArgs, NERModel\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "# from torch.utils.data import IterableDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TokenizedExamplesDataset(IterableDataset):\n",
    "#     def __init__(self, examples):\n",
    "#         self.examples = examples\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         for example in self.examples:\n",
    "#             yield example\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelForInference:\n",
    "    def __init__(self, model_path, max_seq_len=256, use_sliding_window=True, stride=0.8, device='cpu',  batch_size=1): \n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "        self.model.eval()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.use_sliding_window = use_sliding_window\n",
    "        self.stride = stride\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def prepare_example(self, example, tokenizer):\n",
    "        tokenized_example = []\n",
    "        token_mappings = []\n",
    "        window_counts = []\n",
    "\n",
    "        tokens = []\n",
    "        token_id_to_word_id = []\n",
    "        word_id_to_tokenized_len = []\n",
    "        num_windows = 0\n",
    "\n",
    "        for idx, word in enumerate(example):\n",
    "            tokenized_word = tokenizer.tokenize(word)\n",
    "            word_id_to_tokenized_len.append(len(tokenized_word))\n",
    "\n",
    "            if len(tokens) + len(tokenized_word) >= self.max_seq_len - 1:\n",
    "                if not self.use_sliding_window:\n",
    "                    break\n",
    "                tokenized_example.append(\n",
    "                    tokenizer.convert_tokens_to_ids([tokenizer.cls_token] + tokens + [tokenizer.sep_token])\n",
    "                )\n",
    "                num_windows += 1\n",
    "                token_mappings.append([-1] + token_id_to_word_id + [-1])  # encounter for cls and sep tokens\n",
    "\n",
    "                first_word_id = token_id_to_word_id[0]\n",
    "                last_word_id = token_id_to_word_id[-1]\n",
    "                jump_to_id = first_word_id + int((last_word_id - first_word_id) * self.stride)\n",
    "                offset = sum(word_id_to_tokenized_len[first_word_id:jump_to_id])\n",
    "\n",
    "                tokens = tokens[offset:]\n",
    "                token_id_to_word_id = token_id_to_word_id[offset:]\n",
    "\n",
    "            tokens.extend(tokenized_word)\n",
    "            token_id_to_word_id.extend([idx] * len(tokenized_word))\n",
    "\n",
    "        if tokens:\n",
    "            tokenized_example.append(\n",
    "                tokenizer.convert_tokens_to_ids([tokenizer.cls_token] + tokens + [tokenizer.sep_token])\n",
    "            )\n",
    "            num_windows += 1\n",
    "            token_mappings.append([-1] + token_id_to_word_id + [-1])  # encounter for cls and sep tokens\n",
    "\n",
    "        window_counts.append(num_windows)\n",
    "        # print('tokenized_example', tokenized_example)\n",
    "\n",
    "        return tokenized_example, window_counts, token_mappings\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_predictions(windows_preds, windows_scores, windows_token_mappings):\n",
    "        # TODO: refactor\n",
    "        result_size = windows_token_mappings[-1][-2] + 1  # element at -1 corresponds to </s> token, at -2 to last word idx\n",
    "        results = [0] * result_size\n",
    "        result_score = [-100] * result_size\n",
    "\n",
    "        prev_word_id = -1\n",
    "\n",
    "        for window_preds, window_scores, window_token_mappings in \\\n",
    "                zip(windows_preds, windows_scores, windows_token_mappings):\n",
    "            first_word_id=min([x for x in window_token_mappings if x>0])\n",
    "            last_word_id=max(window_token_mappings)\n",
    "            #print('first_word_id',first_word_id)\n",
    "            #print('last_word_id',last_word_id)\n",
    "            for pred, score, word_id in zip(window_preds, window_scores, window_token_mappings):\n",
    "                if word_id == prev_word_id:\n",
    "                    continue  # only check prediction for first token of the word\n",
    "\n",
    "                prev_word_id = word_id\n",
    "                if word_id == -1:\n",
    "                    continue\n",
    "\n",
    "                if pred != 0:\n",
    "                    context=min(word_id-first_word_id, last_word_id-word_id)\n",
    "                    #print('context', context, first_word_id, word_id, last_word_id, pred)\n",
    "                    if context >= result_score[word_id]:\n",
    "                        result_score[word_id] = context\n",
    "                        results[word_id] = pred\n",
    "        return results\n",
    "\n",
    "    def predict(self, to_predict: [str]): \n",
    "        tokenizer = self.tokenizer\n",
    "\n",
    "        example, window_counts, token_mappings = self.prepare_example(to_predict, tokenizer)\n",
    "\n",
    "        tokenized_example = tokenizer.pad({'input_ids': example},  return_tensors='pt') #padding='longest',\n",
    "        # print('tokenized_example', tokenized_example)\n",
    "\n",
    "        preds = []\n",
    "        scores = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "                logits = self.model(**tokenized_example)[0]\n",
    "\n",
    "                batch_score, batch_pred = torch.max(logits, dim=2)\n",
    "\n",
    "                batch_score = batch_score.detach().cpu().numpy()\n",
    "                batch_pred = batch_pred.detach().cpu().numpy()\n",
    "                \n",
    "                scores.extend(batch_score)\n",
    "                preds.extend(batch_pred)\n",
    "                print(preds)\n",
    "\n",
    "        offset = 0\n",
    "\n",
    "        concated_preds = []\n",
    "        for window in window_counts:\n",
    "            concated_preds.append(\n",
    "                self.merge_predictions(preds[offset:offset + window],\n",
    "                                   scores[offset:offset + window],\n",
    "                                   token_mappings[offset:offset + window])\n",
    "            )\n",
    "        print(window_counts)\n",
    "        print(concated_preds)\n",
    "        return concated_preds\n",
    "        # return preds, scores, token_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_labels(preds, labels):\n",
    "    return [labels[p] for p in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_model = '/content/drive/MyDrive/Semestr_III/INL/punctuation_restoration/best_model/'\n",
    "path_to_model = 'best_model'\n",
    "path_to_test = '2021-punctuation-restoration\\\\test-C\\\\in.tsv'\n",
    "path_to_result = 'predictions\\\\test-C-predictions.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelForInference(path_to_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(path_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(path_to_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(path_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = ['B', ':', ';', ',', '.', '-', '...', '?', '!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = \"Szef MSWiA dodał, że operacja policji przebiegła bardzo profesjonalnie, a słowa Marcina Mastalerka, prezydenckiego doradcy, o tym, że Mariusz Kamiński został popchnięty na futrynę, są niepotrzebnym eskalowaniem napięcia. To był niebywały skandal, że posłowie PiS dobijali się do bramy więzienia w sprawie kolegów. Tam ma być cicho i spokojnie, dlatego należy pochwalić decyzję służb o przewiezieniu do innych więzień Kamińskiego i Wąsika - komentuje dr Paweł Moczydłowski, były szef więziennictwa. W rozmowie z WP wskazuje, że planowana w Warszawie demonstracja w obronie prawomocnie skazanych polityków, mogła źle wpłynąć na stan emocjonalny więziennej społeczności. Jak poinformowała Wirtualna Polska, Maciej Wąsik trafił do zakładu karnego w Przytułach Starych niedaleko Ostrołęki, z kolei Mariusz Kamiński do Radomia. Decyzję uzasadniano względami bezpieczeństwa. Od razu okazało się, że pokrzyżowało to plany sympatyków osadzonych w więzieniu polityków. Syn Mariusza Kamińskiego odwołał planowane na czwartek zgromadzenie pod Aresztem Śledczym Warszawa-Grochów, gdzie dotąd przebywali skazani.\"\n",
    "text = \"Szef MSWiA dodał że operacja policji przebiegła bardzo profesjonalnie a słowa Marcina Mastalerka prezydenckiego doradcy o tym że Mariusz Kamiński został popchnięty na futrynę są niepotrzebnym eskalowaniem napięcia To był niebywały skandal że posłowie PiS dobijali się do bramy więzienia w sprawie kolegów Tam ma być cicho i spokojnie dlatego należy pochwalić decyzję służb o przewiezieniu do innych więzień Kamińskiego i Wąsika komentuje dr Paweł Moczydłowski były szef więziennictwa W rozmowie z WP wskazuje że planowana w Warszawie demonstracja w obronie prawomocnie skazanych polityków mogła źle wpłynąć na stan emocjonalny więziennej społeczności Jak poinformowała Wirtualna Polska Maciej Wąsik trafił do zakładu karnego w Przytułach Starych niedaleko Ostrołęki z kolei Mariusz Kamiński do Radomia Decyzję uzasadniano względami bezpieczeństwa Od razu okazało się że pokrzyżowało to plany sympatyków osadzonych w więzieniu polityków Syn Mariusza Kamińskiego odwołał planowane na czwartek zgromadzenie pod Aresztem Śledczym Warszawa Grochów gdzie dotąd przebywali skazani\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = \"Obserwując polityczny rollercoaster ostatnich dni, jak mantrę powtarzamy, że Polska znalazła się na granicy ustrojowego chaosu. A wolne państwo, którego wyglądały kolejne pokolenia, pogrążą się w głębokim kryzysie - pisze dla Wirtualnej Polski prof. Sławomir Sowiński.\"\n",
    "text = \"Obserwując polityczny rollercoaster ostatnich dni jak mantrę powtarzamy że Polska znalazła się na granicy ustrojowego chaosu A wolne państwo którego wyglądały kolejne pokolenia pogrążą się w głębokim kryzysie pisze dla Wirtualnej Polski prof Sławomir Sowiński\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 3, 3, 3, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 4, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0,\n",
      "       0, 4, 0, 4, 0, 0], dtype=int64)]\n",
      "[1]\n",
      "[[0, 0, 0, 0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 5, 0, 0, 0, 0, 4, 0, 4]]\n",
      "Obserwując polityczny rollercoaster ostatnich dni, jak mantrę powtarzamy, że Polska znalazła się na granicy ustrojowego chaosu. A wolne państwo, którego wyglądały kolejne pokolenia, pogrążą się w głębokim kryzysie- pisze dla Wirtualnej Polski prof. Sławomir Sowiński. "
     ]
    }
   ],
   "source": [
    "# one sentence example\n",
    "idx=0\n",
    "output = model.predict(text.split())\n",
    "preds = ids_to_labels(output[0], pred_labels)\n",
    "\n",
    "for _, (word, label) in enumerate(zip(text.split(), preds)):\n",
    "    print(f'{\"\" if idx == 0 else \" \"}{word}{label if label != \"B\" else \"\"}', end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 4, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 3, 0, 0, 4, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 4, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 4]]\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_token_mappings(example, tokenizer):\n",
    "        use_sliding_window = True\n",
    "        max_seq_len = 256\n",
    "        stride = 0.8\n",
    "        token_mappings = []\n",
    "\n",
    "        tokens = []\n",
    "        token_id_to_word_id = []\n",
    "        word_id_to_tokenized_len = []\n",
    "\n",
    "        for idx, word in enumerate(example):\n",
    "            tokenized_word = tokenizer.tokenize(word)\n",
    "            word_id_to_tokenized_len.append(len(tokenized_word))\n",
    "\n",
    "            if len(tokens) + len(tokenized_word) >= max_seq_len - 1:\n",
    "                if not use_sliding_window:\n",
    "                    break\n",
    "                \n",
    "                token_mappings.append([-1] + token_id_to_word_id + [-1])  # encounter for cls and sep tokens\n",
    "\n",
    "                first_word_id = token_id_to_word_id[0]\n",
    "                last_word_id = token_id_to_word_id[-1]\n",
    "                jump_to_id = first_word_id + int((last_word_id - first_word_id) * stride)\n",
    "                offset = sum(word_id_to_tokenized_len[first_word_id:jump_to_id])\n",
    "\n",
    "                tokens = tokens[offset:]\n",
    "                token_id_to_word_id = token_id_to_word_id[offset:]\n",
    "\n",
    "            tokens.extend(tokenized_word)\n",
    "            token_id_to_word_id.extend([idx] * len(tokenized_word))\n",
    "\n",
    "        if tokens:\n",
    "            token_mappings.append([-1] + token_id_to_word_id + [-1])  # encounter for cls and sep tokens\n",
    "\n",
    "        return token_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_mappings = prepare_token_mappings(text, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 50, 51, 52, 54, 55, 56, 57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 93, 95, 96, 98, 99, 100, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 118, 119, 120, 121, 122, 123, 125, 127, 128, 129, 130, 131, 133, 134, 135, 136, 137, 138, 139, 141, 142, 143, 144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157, 159, 160, 161, 162, 163, 164, 165, 167, 168, 169, 170, 171, 172, 173, 174, 175, 177, 178, 179, 180, 181, 182, 183, 185, 186, 187, 189, 191, 192, 193, 194, 195, 196, 197, 198, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 230, 231, 232, 233, 234, 235, 237, 238, 239, 240, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253, 254, 255, 256, 257, 258, -1]]\n"
     ]
    }
   ],
   "source": [
    "print(token_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def merge_predictions(windows_preds, windows_scores, windows_token_mappings):\n",
    "        # TODO: refactor\n",
    "        result_size = windows_token_mappings[-1][-2] + 1  # element at -1 corresponds to </s> token, at -2 to last word idx\n",
    "        results = [0] * result_size\n",
    "        result_score = [-100] * result_size\n",
    "\n",
    "        prev_word_id = -1\n",
    "\n",
    "        for window_preds, window_scores, window_token_mappings in \\\n",
    "                zip(windows_preds, windows_scores, windows_token_mappings):\n",
    "            first_word_id=min([x for x in window_token_mappings if x>0])\n",
    "            last_word_id=max(window_token_mappings)\n",
    "            #print('first_word_id',first_word_id)\n",
    "            #print('last_word_id',last_word_id)\n",
    "            for pred, score, word_id in zip(window_preds, window_scores, window_token_mappings):\n",
    "                if word_id == prev_word_id:\n",
    "                    continue  # only check prediction for first token of the word\n",
    "\n",
    "                prev_word_id = word_id\n",
    "                if word_id == -1:\n",
    "                    continue\n",
    "\n",
    "                if pred != 0:\n",
    "                    context=min(word_id-first_word_id, last_word_id-word_id)\n",
    "                    #print('context', context, first_word_id, word_id, last_word_id, pred)\n",
    "                    if context >= result_score[word_id]:\n",
    "                        result_score[word_id] = context\n",
    "                        results[word_id] = pred\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 3, 3, 3, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 4, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0,\n",
      "       0, 4, 0, 4, 0, 0], dtype=int64)]\n",
      "Obserwując polityczny rollercoaster ostatnich dni jak mantrę powtarzamy że Polska, znalazła się na granicy ustrojowego chaosu, A, wolne, państwo którego wyglądały kolejne pokolenia pogrążą się w głębokim kryzysie. pisze dla Wirtualnej, Polski prof Sławomir Sowiński, "
     ]
    }
   ],
   "source": [
    "idx=0\n",
    "output, scores, _ = model.predict(text.split())\n",
    "output = model(**tokenizer.batch_encode_plus([(text)], padding='longest', add_special_tokens=True, return_tensors='pt'))\n",
    "offset = 0\n",
    "\n",
    "concated_preds = merge_predictions(output[offset:offset + 1], scores[offset:offset + 1], token_mappings[offset:offset + 1])\n",
    "preds = ids_to_labels(concated_preds, pred_labels)\n",
    "\n",
    "for _, (word, (_,label)) in enumerate(zip(text.split(), enumerate(preds))):\n",
    "    print(f'{\"\" if idx == 0 else \" \"}{word}{label if label != \"B\" else \"\"}', end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 4, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(concated_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
